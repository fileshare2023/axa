{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentence transformer model to generate embeddings\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Using a smaller transformer model for speed\n",
        "\n",
        "# Initialize the Hugging Face question answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\", use_auth_token=\"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\")\n",
        "\n",
        "# Step 1: Load content from the provided text files\n",
        "def load_text_files(file_paths):\n",
        "    content = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content.append(file.read().strip())  # Add the cleaned text to the content list\n",
        "    return content\n",
        "\n",
        "# List of file paths\n",
        "files = [\n",
        "    '05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt',\n",
        "    '05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt',\n",
        "    '05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'\n",
        "]\n",
        "\n",
        "# Load the text content from the files\n",
        "document_texts = load_text_files(files)\n",
        "\n",
        "# Step 2: Generate embeddings for the document content\n",
        "document_embeddings = embedder.encode(document_texts)\n",
        "\n",
        "# Step 3: Create a FAISS index for fast retrieval\n",
        "dimension = document_embeddings.shape[1]  # Dimensions of the embeddings\n",
        "index = faiss.IndexFlatL2(dimension)  # Using L2 distance for simplicity\n",
        "index.add(np.array(document_embeddings))  # Add document embeddings to the index\n",
        "\n",
        "# Step 4: Function to retrieve the most relevant document based on the query\n",
        "def retrieve_documents(query, index, embedder):\n",
        "    # Generate query embedding\n",
        "    query_embedding = embedder.encode([query])\n",
        "\n",
        "    # Reshape to (1, dim) for FAISS\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    # Perform the search in FAISS for top 1 relevant document\n",
        "    distances, indices = index.search(query_embedding, k=1)\n",
        "\n",
        "    # Retrieve the relevant document based on index\n",
        "    relevant_doc = document_texts[indices[0][0]]\n",
        "\n",
        "    return relevant_doc\n",
        "\n",
        "# Step 5: Use Hugging Face API to get the answer from the retrieved document\n",
        "def generate_answer(query, relevant_doc):\n",
        "    # Use Hugging Face QA pipeline to get the answer from the relevant document\n",
        "    result = qa_pipeline(question=query, context=relevant_doc)\n",
        "\n",
        "    return result['answer']\n",
        "\n",
        "# Step 6: Putting everything together\n",
        "def answer_query(query, index, embedder, qa_pipeline):\n",
        "    # Retrieve relevant document based on the query\n",
        "    relevant_doc = retrieve_documents(query, index, embedder)\n",
        "\n",
        "    # Generate the final answer using Hugging Face's question-answering pipeline\n",
        "    answer = generate_answer(query, relevant_doc)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example query\n",
        "user_query = \"What is the main focus of the Checks AI project?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = answer_query(user_query, index, embedder, qa_pipeline)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI7yNZC4GG-F",
        "outputId": "74dfe69b-c61c-4569-9959-11cc8992b23b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: privacy rules and regulations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUJNCEZjKG7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-H9gTQJUfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsUkWmwDIdFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkQa1GKsIPsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQF5I8CFHwiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUA0xE23Hg6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}