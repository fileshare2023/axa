{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EuhR9AjwDhZR"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers faiss-cpu torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qt8l_JdUD5ck"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################3"
      ],
      "metadata": {
        "id": "FUv5oWrvEutm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\n",
        "\n",
        "# Load the DPR models and tokenizers\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Load the corresponding tokenizers\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Function to encode query\n",
        "def encode_query(query):\n",
        "    inputs = question_tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        query_embeddings = question_encoder(**inputs).pooler_output\n",
        "    return query_embeddings\n",
        "\n",
        "# Function to encode documents (texts)\n",
        "def encode_documents(documents):\n",
        "    context_embeddings = []\n",
        "    for doc in documents:\n",
        "        inputs = context_tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            context_embedding = context_encoder(**inputs).pooler_output\n",
        "        context_embeddings.append(context_embedding)\n",
        "    return context_embeddings\n",
        "\n",
        "# Load documents from a text file (assuming each line is a separate document)\n",
        "with open('Employee_info.txt', 'r') as file:\n",
        "    documents = file.readlines()\n",
        "\n",
        "# Encode documents\n",
        "document_embeddings = encode_documents(documents)\n",
        "\n",
        "# Example query\n",
        "query = \"least year of experience\"\n",
        "\n",
        "# Encode the query\n",
        "query_embedding = encode_query(query)\n",
        "\n",
        "# Convert torch tensors to numpy arrays\n",
        "query_vector = query_embedding.numpy().flatten()\n",
        "document_vectors = [doc.numpy().flatten() for doc in document_embeddings]\n",
        "\n",
        "# Create a FAISS index for fast retrieval\n",
        "index = faiss.IndexFlatL2(len(query_vector))  # L2 distance metric for cosine similarity\n",
        "\n",
        "# Add document vectors to the index\n",
        "index.add(np.array(document_vectors))\n",
        "\n",
        "# Search the index for the closest match (k=1 to get the best match)\n",
        "D, I = index.search(np.array([query_vector]), k=1)  # k=1 to get the closest document\n",
        "\n",
        "# Retrieve the most relevant document\n",
        "best_document = documents[I[0][0]]\n",
        "print(f\"Most relevant document: {best_document}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ5YiXiMFFb6",
        "outputId": "dfebc2d9-df17-44c7-8aaa-e504056d5f61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant document: Q: Which employee has the least years of experience and what is their designation?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-N0JLxDpFbj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK3UfNGWFHCa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eV5uptTDFMhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQD1aXidD8JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecpRAto1D2da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}