{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6945edc8-07b6-4072-81ed-1c4de18374ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "#!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa27fc5-647b-48eb-9065-dfef545d39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bb5d53-9f37-4e55-ac72-05bc0d6b548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\sriam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure you're using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02daa5da-2774-4e5f-a50a-8e61cfba875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    # Encode user input and convert to tensor\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the original prompt from the response to get a clean reply\n",
    "    return response[len(user_input):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e04e96-3586-40ed-87be-fb59f82f0314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Simple Chatbot using Hugging Face & Gradio\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Message\", placeholder=\"Type a message...\"\n",
    "        )\n",
    "        send_button = gr.Button(\"Send\")\n",
    "    \n",
    "    # Function to handle user interaction\n",
    "    def chat(user_message, history=[]):\n",
    "        response = generate_response(user_message)\n",
    "        history.append((user_message, response))\n",
    "        return history, \"\"\n",
    "\n",
    "    # Connect the Gradio components\n",
    "    send_button.click(chat, [user_input, chatbot], [chatbot, user_input])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19770f13-c1e7-46a4-b3dc-c86b5f513889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
